\documentclass[10pt,a4paper]{article}
\usepackage[francais]{babel}  %Doc fr
\usepackage{fullpage}
\usepackage{euler}
\usepackage{fontspec}
\usepackage{amsmath}
\usepackage{framed}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{graphicx}
\usepackage{hyperref}
\usepackage{enumerate}

%\setmainfont[Numbers=OldStyle]{Linux Libertine O}

\begin{document}
\title{Programmation dynamique}
\author{Paul Melotti \and Thomas Bourgeat\footnote{Vous pouvez bien entendu me contacter par mail thomas.bourgeat@ens.fr, même en cas de début de question : N'HÉSITEZ PAS.}}
\maketitle{}

\section{Multiplications matricielles optimales}
Soit $A_1, \dots, A_n$ des matrices rectangulaires : 
$A_1\in \mathcal{M}_{p_0,p_1}, A_2\in \mathcal{M}_{p_1,p_2}, \dots, A_n\in \mathcal{M}_{p_{n-1},p_n}$.
On cherche à calculer le produit $A_1 \times \dots \times A_n$.
On dispose du tableau $p = [|p_0 ; \dots p_n |]$ des tailles successives.

\paragraph{Question.1}

Combien de multiplications faut-il effectuer pour calculer le produit $A\times B$
où $A\in \mathcal{M}_{p,q}$ et $B\in \mathcal{M}_{q,r}$ ?

Par exemple, si $p=[|10;100;5;50|]$, combien faut-il d'opérations pour calculer
le produit si on le parenthèse en $(A_1 \times A_2) \times A_3$ et en 
$A_1 \times (A_2 \times A_3)$ ?
\\

On constate que l'ordre dans lequel on effectue les multiplications est 
important. L'objectif de ce sujet est de trouver le parenthésage optimal pour 
calculer le produit.

Si $i <= j$, on note $m_{ij}$ le nombre minimal de multiplications scalaires 
pour calculer le produit $A_i \dots A_j$, et $M$ la matrice des
$m_{ij}$ (qui est triangulaire supérieure de diagonale nulle).

Le parenthésage optimal de ce calcul sépare le produit en 
$A_i \dots A_j = (A_i \dots A_k) (A_{k+1} \dots A_j)$ pour un certain indice de
coupure $k\in [i;j-1]$. On note $s_{ij}$ ce $k$, et $S$ la matrice des $s_{ij}$.

\paragraph{Question.2} Montrer que 
\[m_{ij} = \left\{
	\begin{array}{ll}
		0  & \mbox{si } i=j \\
		\min_{i\leq k < j} (m_{ik} + m_{k+1,j} + p_{i-1} p_k p_j) & \mbox{si i < j}.
	\end{array}
\right.\]
Cette formule permet de remplir la matrice $M$ intelligemment si on calcule les
coefficients dans un certain ordre, lequel ?

\paragraph{Question.3} Écrire une fonction \texttt{couts : int vect -> int vect vect}
qui étant donné le vecteur \texttt{p} renvoie la matrice \texttt{M}. Quelle est
sa complexité ?

\paragraph{Question.4} Modifier la fonction précédente en une fonction 
\texttt{coupes : int vect -> int vect vect} qui étant donné \texttt{p} renvoie
la matrice \texttt{S}.

\paragraph{Question.5} Écrire une fonction \texttt{parenthesages : int vect -> unit}
qui étant donné \texttt{p} imprime le parenthésage optimal. Par exemple :
\begin{verbatim}#parenthesages [|3;2;1;4;2;2|] ;;
((1)(2))(((3)(4))(5))- : unit = () \end{verbatim}

\paragraph{Question.6} (*) Combien y a-t-il de façons de parenthéser le produit
$A_1 \dots A_n$ ? Quelle serait la complexité d'un algorithme qui énumèrerait
tous les parenthésages et calculerait le coût de chacun ?

\section{Un peu de réflexion autour de la programmation dynamique}
L'idée de la programmation dynamique est de pouvoir résoudre un problème
d'\emph{optimisation} en
commençant par résoudre des sous-problèmes et en recombinant les solutions de
ces sous-problèmes. 

Pour que celà soit possible, il faut que le problème ait une structure
particulière. Par exemple s'il vérifie la propriété de sous-structure optimale :
lorsqu'on a une solution optimale à notre problème, cette solution contient des
solutions optimales à des sous-problèmes.
Par exemple, pour la section précèdente, trouve un bon parenthèsage pour
$A_1.\dots.A_n$ nous donne un bon parenthèsage pour un couple de 
$A_1.\dots.A_i$ et $A_{i+1}.\dots.A_n$ .

Autrement dit, la solution au gros problème, donne des solutions à des petits
problèmes dont il est composé.


\paragraph{Question.0 (orale)} Parfois les étudiants confondent récursivité et
programmation dynamique. Pensez-y, pourquoi est-ce que ce n'est pas la
même chose, pourquoi est-ce qu'il y pourrait y avoir confusion?

\paragraph{Question.1} Donner un exemple de problème (autre que ceux du sujet),
pour lequel il semble qu'on pourrait faire de la programmation dynamique. Donner
un exemple de problème qui ne s'y prête pas du tout.
\\\\
Rien ne nous dit non plus que l'algorithme qu'on obtient par programmation
dynamique est polynomial. On doit donc faire comme d'habitude des analyses de
complexités.
\paragraph{Question.2} Résoudre par programmation dynamique le problème du rendu
de monnaie optimal : on dispose de pièces/billets (on en a autant qu'on veut!)
des valeurs suivantes : $\{s_1,\dots,s_n\}$, où les $s_i$ sont triées par ordre
croissants. Déterminer le nombre minimum de pièces/billets qu'il faut pour payer
une sommer $s$.  


\section{Distance de Levensthein}

On appelle distance de Levenshtein, ou distance d'édition, la fonction
qui a deux chaines de caractères associe le nombre minimum de
ajout/suppression/modification nécessaires pour passer d'une chaine à l'autre.

\paragraph{Question.1} Est-ce une distance? (Si oui le prouver, si non donner
un contre-exemple).
\paragraph{Question.2} Repérer quel est la propriété de sous-structure qui va
permettre de résoudre le problème par programmation dynamique. Écrire sur papier
votre algorithme. \emph{Indice:} On pourra chercher à calculer tous les
distances pour passer des chaines $s_1 [0..i]$ à $s_2 [0 .. j]$ pour tous les
$i$ et $j$ plus petits que n.
\paragraph{Question.3} Implémenter l'algorithme.


\section{Un dernier pour la route.}
Écrire un algorithme qui trouve la plus longue sous-séquence commune à deux
chaines de caractères, en temps polynomial.
\end{document}
